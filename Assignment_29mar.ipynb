{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02646d71-0d5a-44e8-9d57-076eb07d7d01",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8593846-8e9b-4d0c-b4be-47435dbebf1e",
   "metadata": {},
   "source": [
    "Lasso regression, also known as L1 regularization, is a linear regression technique that helps to prevent overfitting by adding a penalty term to the cost function. The penalty term is proportional to the absolute value of the coefficients of the regression equation, which forces some of them to be zero.\n",
    "\n",
    "The Lasso regression technique is different from other regression techniques like Ridge regression and ordinary least squares regression in the way it handles the coefficients of the regression equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b38267-008f-4f51-8a9b-ecdfe1fb2ddc",
   "metadata": {},
   "source": [
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c28db-d7f1-477b-9055-dd9cb55ca5f2",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection is that it can automatically identify and eliminate irrelevant features by setting their corresponding coefficients to zero. This is because Lasso Regression adds an L1 regularization penalty term to the cost function, which forces some of the coefficients to be zero. As a result, Lasso Regression can perform both feature selection and regularization simultaneously, which can help to improve the predictive performance of the model and make it more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49aa737-02f9-4e0c-a0fe-b8701accb730",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e96a12-a0b3-41bd-8fb4-1f1c6e6cd7be",
   "metadata": {},
   "source": [
    "The coefficients of a Lasso Regression model can be interpreted in a similar way as the coefficients of a standard linear regression model. The coefficient represents the change in the predicted value of the dependent variable (i.e., the target variable) for a unit change in the corresponding independent variable (i.e., the feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f08072-8db6-45c9-bac7-7b127e84b6e3",
   "metadata": {},
   "source": [
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa06741-6c9a-4062-8a9e-930cbd344ed4",
   "metadata": {},
   "source": [
    "Lasso Regression has one primary tuning parameter, which is the regularization parameter alpha. The alpha parameter controls the amount of regularization applied to the regression model.\n",
    "\n",
    "The effect of alpha on the model's performance can be seen in the trade-off between bias and variance. As alpha increases, the model's bias increases because it becomes more constrained and may miss some of the underlying relationships between the features and the target variable. However, the model's variance decreases because it becomes more stable and less likely to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1d5f0-68b3-49ff-be6e-3030b0014cc9",
   "metadata": {},
   "source": [
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4939bf6-a288-42c7-900b-2f40257f3598",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can be used for non-linear regression problems by applying non-linear transformations to the input features.\n",
    "\n",
    "The basic idea is to transform the input features into a higher-dimensional space using non-linear functions, and then apply Lasso Regression to the transformed features. This is known as kernel regression or kernel Lasso Regression. The non-linear functions used to transform the features are called kernels, and they can be chosen based on the specific problem and the properties of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53d859-3fa9-441b-8a65-0e71578427a8",
   "metadata": {},
   "source": [
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967fcfc-2d2f-403f-b7e1-c9cd93bf74d5",
   "metadata": {},
   "source": [
    "- In Ridge Regression, the penalty term is proportional to the square of the magnitude of the coefficients, which is known as L2 regularization. This penalty shrinks the coefficients towards zero but does not force them to be exactly zero. As a result, Ridge Regression can be useful when all of the features are potentially relevant, and it is important to retain all of them in the model, even if some of them have a small effect.\n",
    "\n",
    "- In contrast, Lasso Regression penalizes the absolute value of the coefficients, which is known as L1 regularization. This penalty tends to result in some coefficients being exactly zero, effectively performing feature selection and setting some of the coefficients to exactly zero. As a result, Lasso Regression can be useful when some of the features are irrelevant or redundant and can be eliminated from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a111844-79a8-4dfa-aca7-5bc2d260ed9c",
   "metadata": {},
   "source": [
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf52c7-8226-428f-83e7-1ff23293e189",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features, to some extent. Multicollinearity occurs when there is a high degree of correlation between two or more of the input features, which can cause instability in the estimates of the coefficients and make it difficult to interpret the results of the regression model.\n",
    "\n",
    "Lasso Regression can help address multicollinearity by performing feature selection, which effectively removes redundant or highly correlated features from the model. By setting some of the coefficients to exactly zero, Lasso Regression can identify which features are most important for predicting the target variable and exclude the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e605c-f776-4985-83d0-61e230504fbd",
   "metadata": {},
   "source": [
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3247568-7315-49cb-880e-a4a2f7ad5c7b",
   "metadata": {},
   "source": [
    "The optimal value of the regularization parameter (lambda) in Lasso Regression can be chosen using cross-validation techniques. The basic idea is to train the Lasso Regression model on a training set, with different values of lambda, and evaluate the performance of the model on a validation set. The value of lambda that results in the best performance on the validation set is then chosen as the optimal value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
