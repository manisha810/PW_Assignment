{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc797cd3-f858-49fa-9063-82271676142d",
   "metadata": {},
   "source": [
    "## Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c76d7b-a593-4dc0-a309-ee68d2e30b45",
   "metadata": {},
   "source": [
    "In machine learning algorithms, polynomial functions and kernel functions are both used to transform the input data into a higher-dimensional space. These transformations are commonly employed to enable linear algorithms to learn non-linear relationships between the input variables.\n",
    "\n",
    "Polynomial functions are a type of mapping that can be applied to the input features by raising them to different powers. For example, a polynomial function of degree 2 can transform a two-dimensional input space (x, y) into a six-dimensional space (1, x, y, x^2, y^2, xy). By using polynomial features, linear models can capture non-linear relationships between the variables.\n",
    "\n",
    "Kernel functions, on the other hand, are a mathematical technique used in various machine learning algorithms, such as Support Vector Machines (SVMs) and kernelized versions of Principal Component Analysis (PCA). Kernels allow the algorithms to implicitly operate in a higher-dimensional feature space without explicitly calculating the transformed features. This is known as the \"kernel trick.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f307fef-3bdb-4ef5-a3d8-a980db53374e",
   "metadata": {},
   "source": [
    "## Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4049c6-fcae-469d-8cdb-727e85232f40",
   "metadata": {},
   "source": [
    "To implement an SVM with a polynomial kernel in Python using Scikit-learn, you can follow these steps:\n",
    "\n",
    "- Step 1: Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b6803-75d0-467a-935e-5adcc1a84cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c1a4d-bd9d-4348-870a-02bce1627541",
   "metadata": {},
   "source": [
    "- Step 2: Generate or load your dataset. In this example, we'll generate a synthetic dataset using the make_classification function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79d562-bf3c-468a-adb5-bcba40625ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d39fc-724e-4fae-b758-2b77ef1a79a9",
   "metadata": {},
   "source": [
    "- Step 3: Split the dataset into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d04728-c3b0-467f-8776-8a1658663cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4c3acf-9873-4bac-96c2-04311bb94bb3",
   "metadata": {},
   "source": [
    "- Step 4: Create an instance of the SVM classifier with the polynomial kernel and set the desired parameters. In this case, we'll use a degree-2 polynomial kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342b17f-1798-4bac-9714-144c36aab365",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='poly', degree=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d258e8-53c3-4e5b-ab4a-5a2b359c938c",
   "metadata": {},
   "source": [
    "- Step 5: Train the SVM classifier on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5412191-eaa6-4843-bf39-0c5e9f087171",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77192ce0-d095-414f-ba89-761c38583c30",
   "metadata": {},
   "source": [
    "- Step 6: Make predictions on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abfce5f-56e7-44be-b1c4-a467b925d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f58c31-7cf1-4e9a-9e64-56318ff5cf8d",
   "metadata": {},
   "source": [
    "- Step 7: Evaluate the performance of the SVM by comparing the predicted labels (predictions) with the true labels (y_test), using metrics such as accuracy, precision, recall, or F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3045ab-9afc-4fe6-8c1c-637995240f77",
   "metadata": {},
   "source": [
    "## Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94e1df-9a22-4e1d-ad9e-64ea59cd7f88",
   "metadata": {},
   "source": [
    "Increasing the value of epsilon in SVR has an impact on the number of support vectors. Support vectors are the data points that lie on the margin or violate the margin constraints. When epsilon is increased, the margin around the regression line becomes wider, allowing more data points to fall within the acceptable error range.\n",
    "\n",
    "Here are some observations regarding the effect of increasing epsilon on the number of support vectors in SVR:\n",
    "\n",
    "1. More support vectors: As epsilon increases, the acceptable error range expands, and more data points are considered within the margin. This can result in an increase in the number of support vectors. The wider the margin, the more data points are allowed to be treated as support vectors.\n",
    "\n",
    "2. Increased flexibility: With a larger epsilon, the SVR model becomes more flexible and tolerant to errors. It can accommodate a larger number of data points within the margin without affecting the model's performance or violating the specified epsilon threshold.\n",
    "\n",
    "3. Smoother regression line: Increasing epsilon can result in a smoother regression line since the margin allows more data points to influence the fitting process. This can lead to a less precise but more generalized regression line.\n",
    "\n",
    "4. Longer training time: The number of support vectors affects the training time of the SVR model. As epsilon increases and more data points become support vectors, the complexity of the model increases, potentially leading to longer training times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d3f349-289b-42bf-8380-90ed27d9cf25",
   "metadata": {},
   "source": [
    "## Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea58be8-3029-420e-9465-8493598ab5cf",
   "metadata": {},
   "source": [
    "1. Kernel Function:\n",
    "\n",
    "- SVR allows the use of various kernel functions such as linear, polynomial, radial basis function (RBF), sigmoid, etc.\n",
    "- The kernel function determines how the data is transformed into a higher-dimensional feature space, enabling the model to capture non-linear relationships.\n",
    "- The choice of the kernel function depends on the underlying patterns in the data. \n",
    "\n",
    "- For example:\n",
    "     - Linear kernel: Use when the relationship between the features and the target variable is expected to be linear.\n",
    "     - Polynomial kernel: Use when there are non-linear relationships with higher degrees of interaction among features.\n",
    "    - RBF kernel: Use when the data is expected to have complex non-linear patterns with local variations.\n",
    "    \n",
    "2. C Parameter (Regularization Parameter):\n",
    "\n",
    "- The C parameter controls the trade-off between the complexity of the model and the degree to which errors are tolerated.\n",
    "- A smaller C value allows more errors to be tolerated, resulting in a larger margin and potentially more support vectors.\n",
    "- A larger C value reduces the margin, leading to fewer support vectors and a potentially more complex model that fits the training data more closely.\n",
    "- Increase C when overfitting is a concern, and decrease C when underfitting is observed.\n",
    "\n",
    "\n",
    "3. Epsilon Parameter:\n",
    "\n",
    "- Epsilon (Ïµ) defines the width of the margin around the regression line within which errors are considered acceptable.\n",
    "- It determines the threshold within which data points are not penalized and are considered to lie within the margin.\n",
    "- A larger epsilon allows a wider margin, accommodating more data points within the acceptable error range.\n",
    "- Increase epsilon when a more flexible model is desired, and decrease epsilon when a more precise fitting is required.\n",
    "\n",
    "\n",
    "4. Gamma Parameter:\n",
    "\n",
    "- The gamma parameter controls the influence of each training example on the SVR model.\n",
    "- It determines the reach of the individual training examples in the feature space.\n",
    "- A smaller gamma value considers a larger similarity radius, leading to smoother and more generalized decision boundaries.\n",
    "- A larger gamma value focuses on the nearby data points, resulting in complex and tightly fit decision boundaries.\n",
    "- Increase gamma when overfitting is observed, and decrease gamma when underfitting or a more generalized model is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c55209-11b2-4bdd-9982-1de2938c1143",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "- ###  Import the necessary libraries and load the dataset\n",
    "- ###  Split the dataset into training and testing sets\n",
    "- ###  Preprocess the data using any technique of your choice (e.g. scaling, normaliMation)\n",
    "- ###  Create an instance of the SVC classifier and train it on the training data\n",
    "- ###  hse the trained classifier to predict the labels of the testing data\n",
    "- ###  Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-scoreK\n",
    "- ###  Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performanc_\n",
    "- ###  Train the tuned classifier on the entire dataseg\n",
    "- ###  Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09b4cdf-5639-46a9-9a0e-0482573e9990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best Parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Best Score: 0.9583333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svc_model.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data using standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['rbf', 'linear']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score from GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "svc_tuned = SVC(**best_params)\n",
    "svc_tuned.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(svc_tuned, 'svc_model.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
